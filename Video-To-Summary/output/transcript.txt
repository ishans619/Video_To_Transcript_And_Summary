 Hi, Graffinon. Thank you for coming. So let me start with the machine learning introduction. So before that, I want to introduce our team. So I belong to the Transmission Concentration Technology. And this is who we are. I've been in, you know, most of us. So first, I'm guidelines for these lectures. So I want to go more conceptual. So I don't want to dive into the math and mathematical proofs. There will be some math and formulas, but very light, I will say. And I want to focus more on examples. So it's more, one more due to understand what you can do with that. There will be some hands on, and then we use Python to make the point of the lectures, but not for today, because today is more of an interaction. But what we have for today is what is machine learning, some examples, hard works, what the learning means, some concepts. And for the next session, we are going to discuss the mix and examples in Python. So artificial intelligence, what it is, is many things. It's a could be reasoning, could be knowledge, planning, machine learning, natural language processing, perception. We can move on to the object of the bioavroboral cell. But what is being increasingly popular at this stage are these three. So mostly machine learning, perception, natural language, has been increasingly popular over the last year. And this is the one we are going to go in these sessions. So what's machine learning? I like this definition. It's the science and art of programming computers so that they can learn from data. So there are more formal definitions that I will leave there for you to check. By the way, the slides will be available in ISA Connect. So you don't need to take note of anything. There are some more definitions, but there is a bit more formal and I'll leave for reference if you like to check. So how is different from a computer program? If you like a computer program, you get the data as an input. The program will perform a task with this data. And you get the results as an output. This is a classical program with machine learning. You put some data, you get some results, and instead of being done by a program, it's done by a model. So the difference, because it looked like from this point of view that it's the same, is how the program or the model is built. The program is built by a software developer that takes the requirements and it writes a program. If a machine learning, the way it works is that you provide historical mapping of data and results. So you know already which results correspond to which data and you give this to a model, to train them all and then you can use it. This only works for some task. So I don't see like at this replace in software developers, so it's only for some kind of task. And I want to show you some examples of which kind of task can be done with machine learning. So for instance, Span filter is one of the classical example and one of the oldest. And today we are not getting that much span, I will say. And one of the reason is that most applications are used as Span filter. And the other thing which is also quite useful is a recommendation engine for cross-selling. I don't know if you use an Amazon or Netflix or so. But we have this kind of technology. Predicting for how much property will be sold. So their companies in the real estate market, they predict which would be the sale of a price to identify which are good deals. For something that is selling for less than we should. Advertisement is another classical example. They want to predict which arts, which advertisement I do more likely to click on. Image classification is an interesting one. There is a contest that is designed so that the community of research that is doing image classification can understand better what is in the picture. So the task is to give you a picture and you need to say out of 1000 categories what it is. If it is a ship, if it is a dog, if it is a cat or whatever. And we reach a point where it's much better than humans figuring out what is in the picture. For several cards, the part that is doing the object recognition is also done with machine learning. If you use a CD or Alexa or Google Assistant, the picture recognition and synthesis is also done by machine learning. Language translation. I use it a lot for translating English to German to English. Playing games, I have not used the news about the Alpha Zero Go, also Alpha Go Zero, Alpha Zero AI, and also playing games like Super Mario and so. And they learn how to play just knowing the rules and they develop a strategy on how to play best. Some examples on space, some of you may know it, maybe some of you don't still. So, for instance, Dr. Mast is named after Mast and is a data analytic tool to do automatic diagnostics in case of anomaly. So when there is anomaly, we can automatically identify which are the possible causes from telemetry only. Here I show you an example that I take it from a paper with it full space, so by the time, there was some added error in Venus's press and Dr. Mast found that the reason for that was when a scanner called Asperer was initialized, it was introducing torques in the platform. At that time it was a bit remarkable because nobody was thinking that they were payloads will influence in their platform. So that's nice also about it. Now example in space would be nobody detection and what we realize is that usually what happened before you get an anomaly is that a behavior is unusual. So an unusual behavior is often a precursor if you like a phenomenally. So we look for unusual behavior and tell flight control engineers that something maybe is suspicious. This is an example and while it is simple, I like better because there is no way you can get this in that of limit. For instance, in this case we got the alarm here and two months later we got that all limit. So it's something, maybe it's a this time I tell you it's much better than what it looks like because at that time you don't know how much time you have, but you know already that it's something suspicious. Thermal power consumption. This is a problem for every satellite and I'm looking at red and look that they were very much involved in this project and we together with the advanced concept team organized a machine learning competition and in this machine learning competition the goal was to predict the thermal power consumption for two Martian years and the data we gave them is no, it was a one Martian years so two earth years and we gave them like three Martian years as a data that we need and the results were quite impressive and now they are working and how to make it operational. Another example is the prediction of entry and exit of the radiation belts for integral and for them this is quite relevant because just because of this uncertainty on the effect of the radiation belts, they switch off or they protect the payload earlier than what is really necessary and if they will know in advance with a high accuracy what the right time will be then you could get much science return. This was a nice one. It's not only for nanbo but also for text. This was in collaboration with the communication office and the goal was to predict which articles will receive a higher number of views. So even before publishing in the ISA website an article you will know if many people will read it or not and what we found is these are the topics so if we are talking about ISA company technology is a very low number of views. What we are talking about is set at Comet surface. The operator this kind of fence we have a high number of views so we understand this is what interests people. So in example and I want to mention some consideration when we use Mashi Lenin. First one is that there must be a pattern between the data and the results that we want to extract. If there is no pattern there is nothing to be found so no lottery winning so it cannot predict none of the lottery numbers because there is no pattern. The amount of data should be enough in the sense that we can discover this pattern in comparison with the complexity of the model that we use. It must be difficult to formulate this relationship with a formula because if it would be easy we will use the formula instead. So we are looking for something that cannot be explained with a formula. How the learning happens? What is the learning of a Mashi Lenin? Well we have different Mashi Lenin techniques for different kind of task and the learning is finding which are the model parameters that represent best the input and output mapping. This is maybe the abstract so I prepared an example what does it mean and the easiest thing I could find is to have a linear regression. If you have a linear regression you have two parameters M and B and parameters in the sense that the model is parametrized. So you know that your solution is going to look like a line and learning is finding which are the values of the parameter. And you can have any parameter value but we are looking also to minimize the error. For instance if I have this data I can fit this line. These are the parameter values for M and B and this is what learning means in a very simple model. We can have more complex models for instance we can have polynomials. We can have decision trees and decision trees. The knowledge will be which are the decision we take which are the node we choose. We can have two perfect machines and the parameter we are looking for are the vectors in this case or for instance neural networks and we are looking for which are the values of the weights. The way our solution looks like is very much what we decide it will be like. So if we use a line we can adjust how we would like the parameters of the line but it will be only a line. If we do a tree it will be like it had the shape of having the decisions and nodes and so could be better words but have this way of thinking. So we are a bit constrained by which model we select. We will discuss all these models in the new NX sessions. So what type of learning do we have? And the type of learning depends on the level of supervision we can apply to the model. So we have supervised and supervised and the same supervised and reinforcement learning. For supervised our supervision is that we can tell for every case what the correct answer was. For the case of the thermal power consumption of the master's press we know not in the future but in the past we knew at that time what was the correct thermal power consumption because we can read it for telemetry. So this is what is called supervision at every point we can say this is the error that you are making and this is the right answer. And the focus on supervised learning is to predict the future. So we want to predict the future more than anything else. So sometimes it is difficult to understand what the machine learning is doing and what the model is doing but somehow we can accept not having full understanding because we can more about predicting, rather than understanding what is doing in some cases. And supervised is the opposite. So the level of supervision is that there is no right answer. We are looking for insights while looking at data. And the focus is more understanding the past. Of course you want to understand the past to make improvement for the future but you really want to understand the past because you are not looking for any right answer you are looking for insight. An example would be the market analysis that Supermarket do. For instance now it is quite clear for us that the milk is close to the cheese but it was not always like this. It was discovered that people who buy milk also buy cheese and it was decided that to put them together so to maximize the cross selling. And the supervisor is a bit of a mixture of the two and the supervision is that we can tell what the correct output was for a limited number of cases. For sometimes for a limited number of cases. For instance we are starting our project with the space, the pre-office. And the problem is that we know that for Sentinel 1A there was this particle impact crashing on the solar array and this was in also in telemetry. So you see a drop in power, you see a bit of change of the attitude but there are more things that now we don't know. So based on this non-case we want to understand what it changes in telemetry so that we can detect smaller particles not only in Sentinel 1A but also even other sentinels and use this to calibrate the space debris models for particles. For reinforcement learning the supervision is that we only know the final outcome for instance when playing go or chess and so. We know if you won or if you lost but we cannot really tell which of the steps that you took led to victory or make you lost so we don't know. So the focus here is more to find which is the next action which is most likely to lead you to winning if you like. And the type of learning that is mostly used in the industry, can you guys? Supervised learning and some of the people call it predictive analytics and it's about predicting so predicting but you will buy predicting which advertisement you would click predicting how much this house will be sold for. This is most where the applications are. And depending on what you are predicting we can differentiate two cases so we can have regression or classification and regression is to predict real numbers so if we are going to predict telemetry or we are going to predict how much house will be sold for this what we call regression and classification is predict which option of a limited set of possibilities do we have. For instance for the span filter it's only two possibilities. It's a span or non span. For this case of what is in the picture in the context that was organized you need to predict with objectives out of thousand categories. So I would like to discuss now a bit of the machine learning workflow so if you have a problem that you want to solve with a machine learning which are the steps that you should take. And the first is understanding the problem and I think this one is easy because if you are going to solve a problem you have to understand quite well the problem. Next is to have a evaluation criterion. So how will you measure how good you are solving the problem because otherwise you don't know if it is already solving as your expectation. So are you going to measure maybe how good it is a classifying so you measure accuracy or you measure the error in predicting. Next thing would be a baseline so once you decide which evaluation criteria you have do you know how you are doing it now. So you can measure how you are doing now and how your current baseline measures again this evaluation criteria. Because only then when you go with this machine learning approach you can decide if this is really improving what you are doing or maybe for a 3% improvement I am staying in the way I am because it is simpler maybe. Prepare the data so you need to have enough data and also it is quite some work to do some data transformation. So when you have the data usually you have it in different sources maybe you have telemetry somewhere you have flight dynamics somewhere else or you have some data in websites. And you have data gaps you have out layers you have the data in different sampling rates so you need to put all together and it is not only the data gathering process but it is also that you need to encode your knowledge. For instance if you know that you are predicting the thermal power consumption and the distance to the sun is something that influences. Instead of just putting the distance to the sun you can say hey I know that the square of the distance to the sun to the inverse is or inverse to the square of the distance is actually a better prediction or influences in a way that the model will understand better. So you need to put your knowledge especially if you do not have enough data. Using the data this is a easy part. You define a criteria and you find the parameter that minimize your error. And then you need to do some error analysis. So you need to get it to understand what the model is doing right, what the model is doing wrong. And based on this you might decide that you need a more complex model or maybe that you need a simple model. Or you need more data or you can understand the problem. And once you have done all of that then you realize that your model is ready for production and for production I mean that you want to use it sort of operationally. This I mean that you need to get it to the interfaces and if you get the, if you need to get some data automatically you provide all these interfaces. I would like to discuss a bit this in the middle part mostly the evaluation criteria and the error analysis and put some examples. For evaluation criteria I think this is the only formula we have today. And this is a typical measurement for regression. So if we're going to predict real numbers. And what it means is that this is the estimated value. This is the real value. We square them and we just add together the average. And basically we compare our estimation with the real and this will be the error. The green line. The square to the average. And to make this example I prepared this data that I invented. So I created this line and I put some noise to make it a bit more interesting. And I use two models. The one on the left is a linear regression and the one on the right is a polynomial regression of degree seven. So we compute the error. So this is zero three for the line zero two eight for the polynomial. Therefore the linear is the polynomial and we go for the polynomial. But somehow this doesn't feel right. And the reason it doesn't feel right is because we want machine learning models to generalize. So generalize is that they are able to perform well for data that we never seen. And if we expand a bit the range we see that for linear model it somehow follow our orientation the way it should work. But for a polynomial it's the total failure. So something is going wrong here. And the issue is that for us it's quite easy to see that because we can plot. But usually what happens is that we never have like one dimension or two dimensional data. We have usually many many dimension in machine learning. So we need to figure out how or when this kind of thing is happening that we have a model that is not really serving us in the sense that it will not generalize well. So how to detect that? The solution is that from the data we have we can split the data into. So we can have some data that we use for training a model, for fitting a model. And some other data which is for testing how well we are generalizing. And we are supposed to train the model on trained data only. So when we do this this what happens. And you see that we have a very low error for training and testing. And here the error for testing is quite big. So this is the trick if we know that this is happening this is called overfeating. And when you have overfeating means that your model is too complex for the kind of data you have. So I give you some guidelines for your data. In case you have a training error which is low and a test error which is high, then you need a simple model. And you can also do with more data. And this is called overfeating. You can have the opposite which is that you have a very a model which is underfeating which means that you have a high trained error. It is happen means that your model is not able to learn the data. So you need a more complex model or you need more data. Usually what happens is that maybe what you have is a function of some data and there is some data that you're not putting or that you need to do some more features. This having high trained error and low test error is unusual so it doesn't happen often but I put it there for reference. And when you have a low error everywhere that you're done and you can go and use your model. So if we go effort wise how much effort this thing requires is the problem understanding requires quite some effort. But if it happens that it's your solving a problem that you have, then it's much easier because you have all the knowledge you need. Most of the effort goes on preparing the data. So if you need to have more data or to prepare more features and especially think about what my influence is about the time goes. And of course for production you need to make sure that all interfaces that you need will be there. So you need to interface with telemetry or with flattening mix or the emission planning system and so on. This is also time consuming. So within a we cover a lot today. So now you know what machine learning is, some examples which are the type of machine learning we have, regression classification. And this workflow and very important the generalization problem that we build in models so that we want to predict them well in the future for data that we don't understand yet. So this lecture would be available in the ISA community and the analytics is the link. So you will find it easily. And what we are going to do next is in the second session we are going to dive more on techniques or all the other sessions and more for techniques. And they will have two parts, one part where we discussed how the technology works and then we do some Python hands on. This was a bit more genetic that applies to any kind of technique where we discussed today. So that's from my side. What I wanted to tell you today, thank you for your attention and if you have some questions.