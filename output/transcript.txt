 Apache Kafka, a distributed event streaming platform that can scale massive pipelines of real-time data. It was created in 2011 at LinkedIn, written in Java and Scala, and named Kafka because it's a system optimized for writing. Let's imagine using it to build a dashboard like Google Analytics. When an event occurs, like a website visit, the producer API creates a new record. These records are stored to disk in an ordered, immutable log called a topic, which can persist forever or disappear when no longer needed. Topics are distributed and replicated in a cluster, which contains multiple servers called brokers. This makes Kafka full-tolerant and able to scale to any workload. On the other side, multiple consumers can subscribe to this data. They can read the most recent message like a queue or read the entire topic log and listen to updates in real-time. In addition, it provides a very powerful streams API that can transform and aggregate these topics before they ever reach the consumer. This may sound similar to message brokers like RabbitMQ, but Kafka can handle more throughput and is ideal for streaming data applications. For example, it's used today by companies like LIF to collect and process geolocation data, Spotify and Netflix for log processing, and CloudFlare for real-time analytics. To get started, download it and use a tool like Zookeeper or K-Raff to manage your cluster. Now, in one terminal start Zookeeper, then in the other start the Kafka server. With the environment up and running, we can now create our first topic. Remember a topic is just a log of events kept in order. An event will have a key value and timestamp and may also contain optional metadata and headers. Now, let's use this command to publish an event to the topic, where every line represents a different event. And now these events are written to a topic, which is stored durably and partitioned in the cluster. Kafka guarantees that any consumer of a given topic will always read the events in the exact same order. Now, with this command, we're able to consume the topic. By default, it will give us the latest event, although we can use the from beginning flag to read the entire log. It's also possible to provide an offset to read a subset of records. At this point, we've achieved basic event streaming, but the Kafka Streams API can take things to another level. It's most well supported with Java and can do things like stateless transformation, like filtering out a subset of events, or state full transformation, like an aggregation that combines multiple events into a single value over a certain window of time. And at that point, you're able to manage real-time streams of data at virtually any scale. This has been Apache Kafka in 100 seconds. Hit the like button and subscribe for more short videos like this. Thanks for watching, and I will see you in the next one.